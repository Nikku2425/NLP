{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Import Libraries for Tokenization\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "S4Qq12chcm6W"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import nltk\n",
        "from nltk.tokenize import word_tokenize, sent_tokenize, regexp_tokenize\n",
        "nltk.download('punkt')\n",
        "\n",
        "text = \"using nltk library, which is used for al\"\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "O4ZWc7ylXhfq",
        "outputId": "5d760d36-3a91-45f1-cb8f-3e41b749472a"
      },
      "execution_count": 38,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Word Tokenization"
      ],
      "metadata": {
        "id": "rzh5qq59dxSo"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "words = word_tokenize(text)\n",
        "print(\"Word Tokenization:\")\n",
        "print(words)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Eg8u7_OJXz1s",
        "outputId": "85c4b780-a46b-4c32-ea10-ecfc593d5ea8"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Word Tokenization:\n",
            "['using', 'nltk', 'library', ',', 'which', 'is', 'used', 'for', 'al']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Sent Tokenization"
      ],
      "metadata": {
        "id": "lfp8XBY7dst5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sentences = sent_tokenize(text)\n",
        "print(\"Sentence Tokenization:\")\n",
        "print(sentences)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b0dq0vQLX4Mb",
        "outputId": "da273be9-a8b3-4cff-85b2-3e1581028d7b"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sentence Tokenization:\n",
            "['using nltk library, which is used for al']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Reg Expression"
      ],
      "metadata": {
        "id": "D7wuIm_3dan-"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "formula = r'\\w+'\n",
        "regexp_tokens = regexp_tokenize(text, formula)\n",
        "print(\"RegExp Tokenization:\")\n",
        "print(regexp_tokens)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eFkoS7l_X9oi",
        "outputId": "9c5c5872-f89c-4aa3-9148-a8b1a6c712af"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RegExp Tokenization:\n",
            "['using', 'nltk', 'library', 'which', 'is', 'used', 'for', 'al']\n"
          ]
        }
      ]
    }
  ]
}